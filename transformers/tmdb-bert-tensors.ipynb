{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TMDB BERT Tensors\n",
    "\n",
    "Gives a short intro to using BERT.\n",
    "\n",
    "Runs TMDB overview text data through BERT and saves the output tensors as pickle files for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import torch\n",
    "import numpy\n",
    "import json\n",
    "import datetime\n",
    "import sys\n",
    "import pickle\n",
    "import cupy\n",
    "from numpy.testing import assert_almost_equal\n",
    "is_using_gpu = spacy.prefer_gpu()\n",
    "if is_using_gpu:\n",
    "    print('GPU!')\n",
    "    torch.set_default_tensor_type(\"torch.cuda.FloatTensor\")\n",
    "nlp = spacy.load(\"en_trf_bertbaseuncased_lg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic similarity examples\n",
    "Using huggingface's BERT model and the spacy-pytorch-transformers plugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.73428494\n",
      "768\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Here is some text to encode.\")\n",
    "apple1 = nlp(\"Apple shares rose on the news.\")\n",
    "apple2 = nlp(\"Apple sold fewer iPhones this quarter.\")\n",
    "print(apple1[0].similarity(apple2[0]))\n",
    "print(len(doc.tensor.mean(axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.73428494\n",
      "0.4336571\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Here is some text to encode.\")\n",
    "doc._.trf_word_pieces_  # String values of the wordpieces\n",
    "doc._.trf_word_pieces  # Wordpiece IDs (note: *not* spaCy's hash values!)\n",
    "doc._.trf_alignment  # Alignment between spaCy tokens and wordpieces\n",
    "span = doc[2:4]\n",
    "# .vector and .similarity use the transformer outputs\n",
    "apple1 = nlp(\"Apple shares rose on the news.\")\n",
    "apple2 = nlp(\"Apple sold fewer iPhones this quarter.\")\n",
    "apple3 = nlp(\"Apple pie is delicious.\")\n",
    "print(apple1[0].similarity(apple2[0]))  # 0.73428553\n",
    "print(apple1[0].similarity(apple3[0]))  # 0.43365782"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TMDB Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rawTmdbMovies(filename):\n",
    "    return json.load(open(filename))\n",
    "\n",
    "def indexableMovies(filename='tmdb.json'):\n",
    "    \"\"\" Generates TMDB movies, similar to how ES Bulk indexing\n",
    "        uses a generator to generate bulk index/update actions \"\"\"\n",
    "    tmdbMovies = rawTmdbMovies(filename)\n",
    "    for movieId, tmdbMovie in tmdbMovies.items():\n",
    "        title = ''\n",
    "        overview = ''\n",
    "        if 'title' in tmdbMovie.keys() and isinstance(tmdbMovie['title'], str):\n",
    "            title = tmdbMovie['title']        \n",
    "        if 'overview' in tmdbMovie.keys() and isinstance(tmdbMovie['overview'], str):\n",
    "            overview = tmdbMovie['overview']        \n",
    "        yield movieId,title.strip(),overview.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using tensors for reranking documents\n",
    "\n",
    "For this module, we explore how we can use tensor embeddings that are provided from BERT, to do text similarity.  We'll use this for a reranking demo to see some very interesting results!\n",
    "\n",
    "This notebook only saves the tensors to disk, the notebook 'tmdb-tensor-rerank' then uses these tensors for the search and reranking demo.\n",
    "\n",
    "For each movie, run the overview text through BERT and save the resulting tensor for use in comparissons.  Warning!  This produces lots of data.  Each overview is expanded to a tensor with an average compressed size of 300K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-01 15:02:06.131660\n",
      "2019-11-01 15:02:06.349726\n",
      "0\n",
      "2019-11-01 15:05:01.762650\n",
      "1000\n",
      "2019-11-01 15:07:58.303642\n",
      "2000\n",
      "2019-11-01 15:10:44.196243\n",
      "3000\n",
      "2019-11-01 15:13:30.769849\n",
      "4000\n",
      "2019-11-01 15:16:22.664147\n",
      "5000\n",
      "2019-11-01 15:19:12.125798\n",
      "6000\n",
      "2019-11-01 15:22:12.464227\n",
      "7000\n",
      "2019-11-01 15:25:26.327792\n",
      "8000\n",
      "2019-11-01 15:28:27.298205\n",
      "9000\n",
      "2019-11-01 15:31:30.420762\n",
      "10000\n",
      "2019-11-01 15:34:25.322165\n",
      "11000\n",
      "2019-11-01 15:37:26.900859\n",
      "12000\n",
      "2019-11-01 15:40:29.489763\n",
      "13000\n",
      "2019-11-01 15:43:33.136095\n",
      "14000\n",
      "2019-11-01 15:46:36.670220\n",
      "15000\n",
      "2019-11-01 15:49:21.972174\n",
      "16000\n",
      "2019-11-01 15:52:13.869232\n",
      "17000\n",
      "2019-11-01 15:55:08.838274\n",
      "18000\n",
      "2019-11-01 15:58:11.433081\n",
      "19000\n",
      "2019-11-01 16:01:37.278798\n",
      "20000\n",
      "2019-11-01 16:05:02.406734\n",
      "21000\n",
      "2019-11-01 16:08:15.627856\n",
      "22000\n",
      "2019-11-01 16:11:28.437668\n",
      "23000\n",
      "2019-11-01 16:14:41.070264\n",
      "24000\n",
      "2019-11-01 16:18:08.819549\n",
      "25000\n",
      "2019-11-01 16:21:45.180761\n",
      "26000\n",
      "2019-11-01 16:25:04.495194\n",
      "27000\n"
     ]
    }
   ],
   "source": [
    "movies = []\n",
    "for movieid,title,overview in indexableMovies('../tmdb.json'):\n",
    "    if(len(overview)):\n",
    "        movies.append([movieid,title,overview])\n",
    "\n",
    "i=0\n",
    "print(datetime.datetime.now())\n",
    "for movie in movies:\n",
    "    try:\n",
    "        vectors = cupy.asnumpy(nlp(movie[2]).tensor)\n",
    "        with open('vectors/' + str(movie[0]) + '.pickle','wb') as outfile:\n",
    "            pickle.dump(vectors,outfile)\n",
    "    except:\n",
    "        e = sys.exc_info()[0]\n",
    "        print(i,text[0:24],e)\n",
    "    if i%1000==0:\n",
    "        print(datetime.datetime.now())\n",
    "        print(i)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m = 135\n",
      "n = 768\n",
      "[[-5.1900893e-01 -4.5147708e-01 -1.6509822e-01 ... -4.2124975e-01\n",
      "   7.7184820e-01 -6.4559720e-02]\n",
      " [-8.0238360e-01 -9.1748035e-01 -8.3499974e-01 ...  8.7159008e-02\n",
      "   6.2576568e-01 -5.1558346e-01]\n",
      " [-1.7619932e+00 -2.6495075e+00 -1.4534638e+00 ... -2.4488537e+00\n",
      "   1.1497415e+00 -1.1555958e+00]\n",
      " ...\n",
      " [ 1.1101943e-03 -4.3683273e-01  4.0216300e-01 ...  2.3511750e-01\n",
      "   8.4112855e-03 -9.5739186e-01]\n",
      " [-1.9781429e-01 -5.2316445e-01  1.3382168e-01 ...  1.8816370e-01\n",
      "   3.3785544e-02 -2.9394013e-01]\n",
      " [ 4.6562755e-01 -3.3216560e-01 -2.7836701e-01 ...  9.7025424e-02\n",
      "  -1.6230188e-01 -3.9267188e-01]]\n"
     ]
    }
   ],
   "source": [
    "def examine_tensor(movieid):\n",
    "    tensor=None\n",
    "    with open(\"vectors/\"+movieid+\".pickle\", \"rb\") as input_file:\n",
    "         tensor = pickle.load(input_file)\n",
    "    print('m =',len(tensor))\n",
    "    print('n =',len(tensor[0]))\n",
    "    print(tensor)\n",
    "examine_tensor('100402')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
